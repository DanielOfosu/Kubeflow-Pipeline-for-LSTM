{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"MnhG7TUcrE_U"},"outputs":[],"source":["import kfp\n","import kfp.dsl as dsl\n","from kfp import compiler\n","from kfp import components\n","from kfp.components import InputPath, InputTextFile, OutputPath, OutputTextFile\n","from kfp.components import func_to_container_op"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"dHf08BuSrE_X"},"source":["# PIPELINE"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"yQASPZnbrE_Y"},"outputs":[],"source":["def build_model(X_train_path: InputPath(str), Y_train_path: InputPath(str),\n","         trained_model: OutputPath('TFModel'), epoch_amount:int = 122,batches:int = 64,num_prediction:int =3):\n","    \n","    import pandas as pd\n","    import numpy as np\n","    from tensorflow.keras.models import Sequential\n","    from tensorflow.keras.layers import Dense\n","    from tensorflow.keras.layers import LSTM\n","    from tensorflow.keras.layers import Dropout\n","    from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n","\n","    print('Libraries imported!')\n","    \n","    X_train = np.loadtxt(X_train_path)\n","    X_train = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\n","    Y_train = np.loadtxt(Y_train_path)\n","    \n","    \n","    def build_model(X_train, Y_train,epoch_amount,batches):\n","        \"\"\"Builds the LSTM model\n","\n","        Based on the inputs given starts training the model\n","        in order to be used in forecasts.\n","\n","        Args:\n","            X_train: Training set X, which has been reshaped\n","            Y_train: Training set Y, no modifications\n","\n","        Returns:\n","            model: The final LSTM model\n","            history: Model validation history\n","\n","        Raises:\n","            Exception: Any exception in model training.\n","        \"\"\"\n","        try:\n","            print('Training the LSTM model')\n","\n","            model = Sequential()\n","            model.add(LSTM(100, input_shape=(X_train.shape[1],\n","                                             X_train.shape[2]), recurrent_dropout=0.2))\n","            model.add(Dense(60))\n","            model.add(Dropout(0.2))\n","            model.add(Dense(1))\n","            model.compile(loss='mean_squared_error', optimizer='adam')\n","            # Verbose 0 for cleanliness\n","            history = model.fit(X_train, Y_train, epochs=epoch_amount,\n","                                batch_size=batches, verbose=0, shuffle=False)\n","        except Exception as e:\n","            print(\"Exception in model training \"+ str(e))\n","        else:\n","            print('LSTM Model successfully trained!')\n","            model.save(trained_model)\n","            \n","    build_model(X_train, Y_train, epoch_amount, batches)\n","        \n","   "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"tTs4LEVerE_a"},"outputs":[],"source":["def forecast(station, X_train_path: InputPath(str), model_path: InputPath('TFModel'), dataset_path: InputPath(str),\n","         scaler_path: InputPath(str), df_path: InputPath(str), final_df: OutputPath(str), look_back: int, num_prediction:int =3) -> float:\n","    \"\"\" \n","    Function that predicts future values with given model\n","    \n","    Returns last actual value to help debugging.\n","    \n","    Saves predicted values to a bigQuery table.\n","    \"\"\"\n","        \n","    import sys, subprocess;\n","    subprocess.run([sys.executable, '-m', 'pip', 'install', 'protobuf==3.12.0'])\n","    subprocess.run([sys.executable, '-m', 'pip', 'install', 'joblib==0.16.0'])\n","    subprocess.run([sys.executable, '-m', 'pip', 'install', 'grpcio==1.24.3'])\n","    import pandas as pd\n","    import numpy as np\n","    import joblib\n","    import datetime\n","    from datetime import timedelta\n","    from time import time\n","    import tensorflow as tf\n","    from tensorflow.keras.models import Sequential\n","    from sklearn.preprocessing import MinMaxScaler\n","    import time\n","    from dateutil.relativedelta import relativedelta\n","\n","    print('Libraries imported!')\n","    \n","    X_train = np.loadtxt(X_train_path)\n","    X_train = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\n","    model = tf.keras.models.load_model(model_path)\n","    dataset = np.loadtxt(dataset_path)\n","    scaler = joblib.load(scaler_path)\n","    df1 = pd.read_csv(df_path)\n","    print(df1.tail())\n","    df1['Date'] = pd.to_datetime(df1['Date'])\n","    \n","        \n","    def forecasting(X_train,dataset,scaler,model,num_prediction):\n","\n","        def predict(num_prediction, model):\n","                \"\"\"Creates a prediction based on the model given\n","\n","                Makes a forecast for the time steps given on num_prediction. \n","                For example num_prediction = 3 outputs t, t+1 ,t+2 and t+3 values, \n","                of which the three latter are forecasts.\n","\n","                Args:\n","                    num_prediction: How many timesteps forward is the model predicting\n","                    model: The pretrained model which is used in the prediction\n","\n","                Returns:\n","                    prediction_list: A list of the predicted values\n","\n","\n","                Raises:\n","                    Exception: Any exception in model training.\n","                \"\"\"\n","                try:\n","                    print('Making the predictions')\n","                    prediction_list = X_train[-look_back:]\n","\n","                    for _ in range(num_prediction):\n","                        x = prediction_list[-look_back:]\n","                        x = x.reshape((1, 1, look_back))\n","                        out = model.predict(x)[0][0]\n","                        prediction_list = np.append(prediction_list, out)\n","                    prediction_list = prediction_list[look_back-1:]\n","                except Exception as e:\n","                    print(\"Exception in prediction\" + str(e))\n","                else:\n","                    return prediction_list\n","        # Reshaping the dataset in preparation to apply the forecast\n","        X_train = dataset.reshape((-1))\n","\n","        # How many time-steps forward is being predicted.\n","        forecast = predict(num_prediction, model)\n","        forecast = scaler.inverse_transform([forecast])\n","        print('Forecasting successful!')\n","        # The first value is the last actual value.\n","        # The last three values are predictions (forecasts).\n","        # t, t+1 ,t+2 ,t+3\n","        lastmonth = df1['Date'][len(df1) - 1].date()\n","        nextmonth = lastmonth.replace(day=1) + relativedelta(months=1)\n","        print(\"lastmonth\",lastmonth)\n","        print(\"nextmonth:\",nextmonth)\n","        final = pd.DataFrame(columns=['Date','Prediction','Model','MET_LOAD_TIME','MET_CRT_BY_PROCESS', 'Station'])\n","\n","        for i in range(num_prediction):\n","            current = datetime.datetime.now()\n","\n","            final.at[i,'Date'] = nextmonth + relativedelta(months=i)\n","            final.at[i,'Prediction'] = forecast[0][i+1]\n","            final.at[i,'Model'] = 'LSTM'\n","            final.at[i,'MET_LOAD_TIME'] = current\n","            final.at[i,'MET_CRT_BY_PROCESS'] = 'LSTM Process'\n","            final.at[i, 'Station']  = station\n","\n","        \n","        final.to_csv(final_df, index=False)\n","        return(forecast[0][0])\n","    \n","    # Function call\n","    return(forecasting(X_train,dataset,scaler,model,num_prediction))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"RuXnnB8urE_c"},"outputs":[],"source":["def write_data(result_path: InputPath(str)):\n","    \"\"\"Writes result to BigQuery.\n","    \n","    Arguments: Path to result dataframe.\n","    \"\"\"\n","    import sys, subprocess;\n","    import pandas as pd\n","    import io\n","    subprocess.run([sys.executable, '-m', 'pip', 'install', 'pandas_gbq==0.13.2'])\n","    subprocess.run([sys.executable, '-m', 'pip', 'install', 'google-auth<2.0dev,>==1.18.0'])\n","    subprocess.run([sys.executable, '-m', 'pip', 'install', 'pyarrow==0.17.1'])\n","    import pandas_gbq\n","    import pyarrow\n","    from dateutil.relativedelta import relativedelta\n","    from google.cloud import bigquery\n","    \n","    final = pd.read_csv(result_path)\n","    print(\"Result table read, starting upload to BQ.\")\n","    client = bigquery.Client()\n","    table_id = 'r-instance.CL_Demand_Forecast.CL_demand_predictions'\n","    # Since string columns use the \"object\" dtype, pass in a (partial) schema\n","    # to ensure the correct BigQuery data type.\n","    \n","    job_config = bigquery.LoadJobConfig()\n","    job_config.source_format = bigquery.SourceFormat.NEWLINE_DELIMITED_JSON\n","    job_config.schema=[\n","        bigquery.SchemaField('Date', bigquery.enums.SqlTypeNames.DATE),\n","        bigquery.SchemaField('Prediction', bigquery.enums.SqlTypeNames.FLOAT),\n","        bigquery.SchemaField('Model', bigquery.enums.SqlTypeNames.STRING),\n","        bigquery.SchemaField('MET_LOAD_TIME', bigquery.enums.SqlTypeNames.DATETIME),\n","        bigquery.SchemaField('MET_CRT_BY_PROCESS', bigquery.enums.SqlTypeNames.STRING),\n","        bigquery.SchemaField('Station', bigquery.enums.SqlTypeNames.STRING),\n","    ]\n","    print(str(final.to_json(orient=\"records\",lines=True)))\n","    with io.StringIO(final.to_json(orient=\"records\",lines=True)) as source_file:\n","        job = client.load_table_from_file(source_file, table_id, job_config=job_config)\n","    \n","    # Wait for the load job to complete.\n","    job.result()\n","    print(\"Table uploaded to bigQuery!\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"imaV6_7yrE_f"},"outputs":[],"source":["def query_data(data_path: OutputPath(str), extdata_path: OutputPath(str), datas1_path: OutputPath(str), datas2_path: OutputPath(str), datas3_path: OutputPath(str), datas4_path: OutputPath(str)):\n","    \"\"\"Queries the data from BigQuery.\n","\n","    Loads the data from BigQuery.\n","\n","\n","    Returns:\n","        df1: A pandas dataframe of the CardLock data\n","\n","    Raises:\n","        NotFound: An error occured in loading the table from BQ.\n","    \"\"\"\n","    import sys, subprocess;\n","    subprocess.run([sys.executable, '-m', 'pip', 'install', 'pandas_gbq==0.13.2'])\n","    subprocess.run([sys.executable, '-m', 'pip', 'install', 'grpcio==1.24.3'])\n","    subprocess.run([sys.executable, '-m', 'pip', 'install', 'google-auth==1.18.0'])\n","    subprocess.run([sys.executable, '-m', 'pip', 'install', 'google-auth<2.0dev,>==1.18.0'])\n","    import pandas as pd\n","    import pandas_gbq\n","    from google.cloud import bigquery\n","    from google.api_core.exceptions import AlreadyExists, NotFound\n","    try:\n","        # Loads the data from BigQuery\n","        # Expects that Transaction_Date is stored as datetime and Quantity as float in BigQuery.\n","        print('Loading the data...')\n","        client = bigquery.Client()\n","        sql1 = \"\"\"\n","   HIDDEN\n","        \"\"\"\n","\n","        df1 = client.query(sql1).to_dataframe()\n","        df1 = df1.rename(columns={\"Quantity\": \"Amount\"})\n","        \n","        # Loads the historical demand from bigquery\n","        sql2 = \"\"\"\n","HIDDEN\n","        \"\"\"\n","        df2 = client.query(sql2).to_dataframe()\n","        \n","\n","        sql_station_1 = \"\"\"\n","HIDDEN\n","        \"\"\"\n","        sql_station_2 = \"\"\"\n","HIDDEN\n","        \"\"\"\n","        sql_station_3 = \"\"\"\n","HIDDEN\n","        \"\"\"\n","        sql_station_4 = \"\"\"\n","HIDDEN\n","        \"\"\"\n","        client = bigquery.Client()\n","        df_station1 = client.query(sql_station_1).to_dataframe()\n","        df_station2 = client.query(sql_station_2).to_dataframe()\n","        df_station3 = client.query(sql_station_3).to_dataframe()\n","        df_station4 = client.query(sql_station_4).to_dataframe()\n","        print('Data loaded')\n","\n","    except NotFound:\n","        print(\"An error occured in loading the tables from BigQuery: TABLE NOT FOUND\")\n","    else:\n","        #return df1, df2\n","        df1.to_csv(data_path, index=False)\n","        df2.to_csv(extdata_path, index=False)\n","        df_station1.to_csv(datas1_path, index=False)\n","        df_station2.to_csv(datas2_path, index=False)\n","        df_station3.to_csv(datas3_path, index=False)\n","        df_station4.to_csv(datas4_path, index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"ZIxLBL5KrE_h"},"outputs":[],"source":[" def process_data(station, look_back, data_path: InputPath(str), datas1_path: InputPath(str), datas2_path: InputPath(str), datas3_path: InputPath(str), datas4_path: InputPath(str), edata_path: InputPath(str), xtrain_path: OutputPath(str), ytrain_path: OutputPath(str),\n","                  dset_path: OutputPath(str), scaler_path: OutputPath(str), df_path: OutputPath(str)):\n","    \"\"\"Preprocess the data to be ready for applying the LSTM model\n","\n","    Args:\n","        df1: A pandas dataframe of the Cardlock company data\n","        df2: A pandas dataframe of the historical US data\n","        look_back: How many months of data the model needs for the predictions\n","\n","    Returns:\n","        X_train: Training set X, which has been reshaped\n","        Y_train: Training set Y, no modifications\n","        dataset: The full dataset\n","        scaler: The scaler used, which can be used later on to invert the scaling\n","\n","    Raises:\n","        IndexError: Sequence subscript is out of range.\n","    \"\"\"\n","    import sys, subprocess;\n","    subprocess.run([sys.executable, '-m', 'pip', 'install', 'joblib==0.16.0'])\n","    subprocess.run([sys.executable, '-m', 'pip', 'install', 'grpcio==1.24.3'])\n","    subprocess.run([sys.executable, '-m', 'pip', 'install', 'pyarrow==0.17.1'])\n","    subprocess.run([sys.executable, '-m', 'pip', 'install', 'pandas_gbq==0.13.2'])\n","    import pandas as pd\n","    import numpy as np\n","    #import datetime\n","    import joblib\n","    #from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n","    from sklearn.preprocessing import MinMaxScaler\n","    import time\n","    import pyarrow\n","    from google.cloud import bigquery\n","    from google.api_core.exceptions import AlreadyExists, NotFound\n","    \n","\n","    def check_values(df):  \n","        if any(df['Amount'] < 0):\n","            print('WARNING: There are negative values in the data!')\n","\n","        if any(df['Amount'] > 1000000):   #values over 1 million\n","            print('WARNING: The data contains values over a million.')\n","        else:\n","            print('No outlier values found!')\n","    \n","    try:\n","        print('Preprocessing the data for LSTM modeling')\n","        df1 = pd.read_csv(data_path)\n","        df2 = pd.read_csv(edata_path)\n","        df_station1 = pd.read_csv(datas1_path)\n","        df_station2 = pd.read_csv(datas2_path)\n","        df_station3 = pd.read_csv(datas3_path)\n","        df_station4 = pd.read_csv(datas4_path)\n","        \n","        stations = [\"HIDDEN\"]\n","        \n","        if(station==\"HIDDEN\"):\n","            df1 = df_station1\n","        if(station==\"HIDDEN\"):\n","            df1 = df_station2\n","        if(station==\"HIDDEN\"):\n","            df1 = df_station3\n","        if(station==\"HIDDEN\"):\n","            df1 = df_station4\n","        \n","        print(\"Dataframes ready.\")\n","        \n","        # The next part is for checking if the last month has full data or not.\n","\n","\n","        # Sets the latest month as being full\n","        latest_month_full = True\n","\n","        # If the latest month is not full, set latest_month_full to False.\n","        def check_full_month(df):\n","            import datetime\n","            import calendar\n","            df['Date'] = pd.to_datetime(df['Date'])\n","            df = df.set_index('Date')\n","            df = df.asfreq(df.index.freq)\n","            \n","            df['day'] = df.index.day\n","            last_date = df['day'].iloc[-1]\n","            \n","            # Marks the month as being full if the date is 29 30 or 31\n","            # TODO: Check the last date properly\n","            if last_date < 28:\n","                return False\n","     \n","        latest_month_full_station1 = check_full_month(df_station1)\n","        latest_month_full_station2 = check_full_month(df_station2)\n","        latest_month_full_station3 = check_full_month(df_station3)\n","        latest_month_full_station4 = check_full_month(df_station4)\n","   \n","        check_values(df1)\n","\n","        def create_dataset(dataset, look_back):\n","            X, Y = [], []\n","            for i in range(len(dataset)-int(look_back)-1):\n","                a = dataset[i:(i+int(look_back)), 0]\n","                X.append(a)\n","                Y.append(dataset[i + int(look_back), 0])\n","            return np.array(X), np.array(Y)\n","        \n","        if station in stations:\n","            latest_month_full = check_full_month(df1)\n","        else:\n","            # Drops the last row if last month is not full of data\n","            if latest_month_full_station1 == False or latest_month_full_station2 == False or latest_month_full_station3 == False or latest_month_full_station4 == False:\n","                latest_month_full = False\n","\n","        # Modifies the data to Monthly data through resampling\n","        df1['Date'] = pd.to_datetime(df1['Date'])\n","        df1 = df1.set_index('Date')\n","        df1 = df1.asfreq(df1.index.freq)\n","        df1 = df1.resample('M').sum()\n","        \n","\n","        if latest_month_full == False:\n","            df1 = df1[:-1]\n","        \n","        # Dropping unneeded dates\n","        df2 = df2.set_index('Date')\n","        df2 = df2[-168:-27]\n","        df2 = df2.sort_index()\n","\n","        # TODO: This row drops the last 5 rows due to missing data. Remove in final version\n","        #df1 = df1[:-5]\n","        # Dropping the first 11 rows in order to get a more accurate representation of the real situation\n","        df1 = df1[11:]\n","        print(df1.tail(5))\n","        # Placeholders for debugging\n","        # df2.to_csv('LSTM_debug/df2.csv', index = True, header=True)\n","        # df1.to_csv('LSTM_debug/df1.csv', index = True, header=True)\n","        \n","        #testchanges\n","        dfmodified = df1.reset_index()\n","        dfmodified.to_csv(df_path, index=False)\n","        # Scaling both of the datasets with the MinMaxScaler\n","        historical_data = df2.values  # numpy.ndarray\n","        historical_data = historical_data.astype('float32')\n","        historical_data = np.reshape(historical_data, (-1, 1))\n","        scaler2 = MinMaxScaler(feature_range=(0, 1))\n","        historical_data = scaler2.fit_transform(historical_data)\n","\n","        data = df1.values  # numpy.ndarray\n","        data = data.astype('float32')\n","        data = np.reshape(data, (-1, 1))\n","        scaler = MinMaxScaler(feature_range=(0, 1))\n","        data = scaler.fit_transform(data)\n","\n","        # Combining the scaled datasets\n","        dataset = np.concatenate([historical_data, data])\n","        # pd.DataFrame(dataset).to_csv(\"test.csv\")\n","\n","        X_train, Y_train = create_dataset(dataset, look_back)\n","        # Reshaping input to be [samples, time steps, features]\n","        #X_train = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\n","        \n","\n","    except IndexError:\n","        print(\"Out of index: look back time is too long\")\n","    else:\n","        print('Data successfully preprocessed')\n","        np.savetxt(xtrain_path, X_train)\n","        np.savetxt(ytrain_path, Y_train)\n","        np.savetxt(dset_path, dataset)\n","        joblib.dump(scaler, scaler_path)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"13-aNbxCrE_j"},"outputs":[],"source":["def evaluate_model(dataset_path: InputPath(str), scaler_path: InputPath(str), look_back: int, epoch_amount:int = 122,batches:int = 64) -> float:\n","    \"\"\"Function to evaluate model performance with past data.\n","    \n","    Conduct a train-test split and evaluate performance with test data.\n","    \n","    Args: Some data\n","    \n","    Returns: MAE metric for the test period.\n","    \"\"\"\n","    import sys, subprocess;\n","    subprocess.run([sys.executable, '-m', 'pip', 'install', 'joblib==0.16.0'])\n","    import joblib\n","    import pandas as pd\n","    import numpy as np\n","    from tensorflow.keras.models import Sequential\n","    from tensorflow.keras.layers import Dense\n","    from tensorflow.keras.layers import LSTM\n","    from tensorflow.keras.layers import Dropout\n","    from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n","    from sklearn.preprocessing import MinMaxScaler\n","    from sklearn.metrics import mean_squared_error\n","    from sklearn.metrics import mean_absolute_error\n","    \n","    print(\"Libraries imported!\")\n","    \n","    scaler = joblib.load(scaler_path)\n","    dataset = np.loadtxt(dataset_path)\n","    print(dataset.shape)\n","    print(len(dataset))\n","    train_size = int(len(dataset) * 0.80)\n","    test_size = len(dataset) - train_size\n","    train, test = dataset[0:train_size], dataset[train_size:len(dataset)]\n","    def create_dataset(dataset, look_back=18):\n","        X, Y = [], []\n","        for i in range(len(dataset)-look_back-1):\n","            a = dataset[i:(i+look_back)]\n","            X.append(a)\n","            Y.append(dataset[i + look_back])\n","        return np.array(X), np.array(Y)\n","    print(\"Creating dataset!\")\n","    #look_back = 5\n","    X_train, Y_train = create_dataset(train, look_back)\n","    X_test, Y_test = create_dataset(test, look_back)\n","\n","    # reshape input to be [samples, time steps, features]\n","    X_train = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\n","    print(\"Reshaping train.\")\n","    X_test = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))\n","    print(\"Reshaping test.\")\n","    \n","    model = Sequential()\n","    model.add(LSTM(100, input_shape=(X_train.shape[1],\n","                                     X_train.shape[2]), recurrent_dropout=0.2))\n","    model.add(Dense(60))\n","    model.add(Dropout(0.2))\n","    model.add(Dense(1))\n","    model.compile(loss='mean_squared_error', optimizer='adam')\n","    # Verbose 0 for cleanliness\n","    history = model.fit(X_train, Y_train, epochs=epoch_amount,\n","                        batch_size=batches, verbose=0, shuffle=False)\n","    print(\"Model fitted.\")\n","\n","    test_predict = model.predict(X_test)\n","    test_predict = scaler.inverse_transform(test_predict)\n","    Y_test = scaler.inverse_transform([Y_test])\n","    print('Test Mean Absolute Error:', mean_absolute_error(Y_test[0], test_predict[:,0]))\n","    print('Test Root Mean Squared Error:',np.sqrt(mean_squared_error(Y_test[0], test_predict[:,0])))\n","    return(mean_absolute_error(Y_test[0], test_predict[:,0]))\n","    "]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Jg7Z2Y2LrE_l"},"source":["## Building the pipeline\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"tVTQ_BvOrE_l"},"outputs":[],"source":["# Convert the functions to pipeline operations.\n","query_op = components.func_to_container_op(\n","    query_data,\n","    base_image='jupyter/tensorflow-notebook:54462805efcb'\n",")\n","\n","process_data_op = components.func_to_container_op(\n","    process_data,\n","    base_image='jupyter/tensorflow-notebook:54462805efcb'\n",")\n","\n","build_model_op = components.func_to_container_op(\n","    build_model,\n","    base_image='jupyter/tensorflow-notebook:54462805efcb'\n",")\n","predict_op = components.func_to_container_op(\n","    forecast,\n","    base_image='jupyter/tensorflow-notebook:54462805efcb'\n",")\n","write_op = components.func_to_container_op(\n","    write_data,\n","    base_image='jupyter/tensorflow-notebook:54462805efcb'\n",")\n","\n","evaluate_op = components.func_to_container_op(\n","    evaluate_model,\n","    base_image='jupyter/tensorflow-notebook:54462805efcb'\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"TXjbKmCBrE_o"},"outputs":[],"source":["# Create pipeline.\n","@dsl.pipeline(\n","   name='LSTM pipeline',\n","   description='A pipeline that creates a forward forecasting prediction with LSTM modelling.'\n",")\n","def lstm_pipeline(\n","    epoch_amount: int =122,\n","    batches: int =64,\n","    num_prediction: int =3,\n","    look_back: int =18,\n","    station: str=\"all\"\n","):\n","    data = query_op()\n","    processed = process_data_op(station, look_back, data.outputs['data'], data.outputs['datas1'], data.outputs['datas2'], data.outputs['datas3'], data.outputs['datas4'], data.outputs['extdata'])\n","    mae = evaluate_op(processed.outputs['dset'], processed.outputs['scaler'], look_back, epoch_amount, batches)\n","    model = build_model_op(processed.outputs['xtrain'], processed.outputs['ytrain'], \n","                           look_back,epoch_amount,batches)\n","    \n","    \n","    preds = predict_op(station, processed.outputs['xtrain'], model.outputs['trained_model'], processed.outputs['dset'],\n","                           processed.outputs['scaler'], processed.outputs['df'], look_back=look_back, num_prediction=num_prediction)\n","    data.execution_options.caching_strategy.max_cache_staleness = \"P1D\"\n","    processed.execution_options.caching_strategy.max_cache_staleness = \"P1D\"\n","    mae.execution_options.caching_strategy.max_cache_staleness = \"P1D\"\n","    model.execution_options.caching_strategy.max_cache_staleness = \"P1D\"\n","    preds.execution_options.caching_strategy.max_cache_staleness = \"P1D\"\n","    write_op(preds.outputs['final_df'])\n","    \n","    \n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"M9ZDw5U2rE_q"},"source":["## Compile and run the pipeline"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"JZeY3f-xrE_q"},"outputs":[],"source":["# Compile the pipeline\n","compiler.Compiler().compile(lstm_pipeline, 'lstm_pipeline.tar.gz')\n","\n","# Pipeline Argument Values\n","arguments = {'epoch_amount': '122', 'batches': '64', 'num_prediction': '3', 'look_back': '18', 'station': 'Address_here'}\n","\n","kfpclient = kfp.Client(host='HIDDEN')\n","\n","\n","exp = kfpclient.create_experiment(name='LSTM model experiment')\n","run = kfpclient.run_pipeline(exp.id, 'LSTM run test', 'lstm_pipeline.tar.gz', params=arguments)\n","\n","\n","\n","# The generated link below leads to the pipeline run information page."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"eF_ou2sCrE_t"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"CIjry-WwrE_v"},"outputs":[],"source":[]}],"metadata":{"colab":{"name":"LSTM Kubeflow Pipeline.ipynb","provenance":[]},"environment":{"name":"r-cpu.3-6.m46","type":"gcloud","uri":"gcr.io/deeplearning-platform-release/r-cpu.3-6:m46"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"nbformat":4,"nbformat_minor":0}
